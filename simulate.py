from ast import Or
from tracemalloc import start
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import copy
import random
import math
import scipy.stats as st
from probabilities import *
from collections import Counter

twitter_datas = pd.read_csv("twitter.csv")
twitter_datas.columns = ['Time', 'QPS']
perform_datas = pd.read_csv("perform.csv")
# pd.options.display.float_format = '{:.10f}'.format


lambda_data_2048 = {
    'mobilenet_v2': 43.6,
    'inception_v3': 275.5,
    'resnet50': 193,
    'vgg16': 466,
    'vgg19': 554,
}
lambda_price_2048 = 0.0000000333


inferentia_price = 0.0060333333 / 60

datas = twitter_datas

x = list(datas.get('Time').values)
y = list(datas.get('QPS').values)

print(datas['QPS'][:600].sum())

totals = datas['QPS'][:600].sum()

TARGET_LATENCY = 200  # ms
T = [1, 5, 10, 30, 60, 180]

default = {}
for t in T:
    default[t] = 0

inferentia_price = 0.000100555555
model = "resnet50"

model_index = {
    "mobilenet_v2": 0,
    "inception_v3": 1,
    "resnet50": 2,
    "vgg16": 3,
    "vgg19": 4
}

df = perform_datas[perform_datas["models"].isin([model])]
inferentia_per_second = int(df['InferentiaPerform'].values[0] / 60)
lambda_per_second = int(df['LambdaPerform'].values[0] / 60)

lambda_per_price = float(df['LambdaEventPerPrice'].values[0])

PerformInferentia = copy.deepcopy(default)
for t in T:
    PerformInferentia[t] = inferentia_per_second * t

PerformLambda = copy.deepcopy(default)
for t in T:
    PerformLambda[t] = lambda_per_second * t

# print("Perform_Inferentia:", PerformInferentia)
# print("Perform_Lambda:", PerformLambda)

InferentiaInstances = 0
InferentiaJobs = 0
LambdaWorkers = 0

Instance_Cold_Start = 10
EVENT_TESTING = 1

WorkedByLambda = []
WorkedByInferentia = []


# Time Step 별 도착하는 Event 양 체크
def RequestMonitor(start_time):
    requests = copy.deepcopy(default)
    for t in T:
        # 현재 시간 부터 앞으로의 t 만큼 체크, 이벤트의 총합 계산
        requests[t] = twitter_datas[
            start_time:start_time + t]['QPS'].values.sum(axis=0) * EVENT_TESTING
    return requests


start_time = 0
end_time = 599

instance_start_time = 0
SIMULATIONS = 10
InstanceOnTimes = 0


def OnlyLambda(currentJobs, events):
    currentJobs += events[0]
    return currentJobs


def MaxInstance(NeedInstances, events):
    while events[0] > inferentia_per_second * NeedInstances:
        NeedInstances += 1

    return NeedInstances


def ScalingInstances(UsedInstances, events):
    instances = 0
    while True:
        RemainEvents = events - \
            np.array(list(PerformInferentia.values())) * (instances + 1)
        EventRatio = len(RemainEvents[RemainEvents < 0]) / len(RemainEvents)
        if EventRatio > 0.5:
            break
        else:
            instances += 1

    instances += 1
    UsedInstances += instances
    return UsedInstances


def OptimizeSystem(LambdaJobs, InferentiaInstances, events):
    CurrentLambdaJobs = 0
    TotalEventValues = events
    NeedInstances = 0
    while True:
        RemainEvents = TotalEventValues - \
            np.array(list(PerformInferentia.values())) * (NeedInstances + 1)
        EventRatio = len(RemainEvents[RemainEvents < 0]) / len(RemainEvents)
        if EventRatio > 0.5:
            break
        else:
            NeedInstances += 1

    if NeedInstances == 0:
        RemainEvents = TotalEventValues
    else:
        RemainEvents = TotalEventValues % (
            NeedInstances * np.array(list(PerformInferentia.values())))

    PreferedLambda = np.array(list(PerformLambda.values())) > RemainEvents
    LambdaRatio = len(
        PreferedLambda[PreferedLambda == True]) / len(PreferedLambda)
    #     print("LambdaRatio:", LambdaRatio)

    LambdaUsed = False
    if LambdaRatio <= 0.5:
        NeedInstances += 1
    else:
        LambdaUsed = True

    CurrentInferentiaJobs = events[0]

    if InferentiaInstances > 0:
        time_idx = 0
        for TimeKey, SIM_TIME in PerformInferentia.items():
            SERVERS = NeedInstances
            MU = SIM_TIME / TimeKey  # 1/mu is exponential service times
            LAMBDA = TotalEventValues[time_idx] / TimeKey
            RHO = LAMBDA / (MU * SERVERS)

            W = expw(MU, SERVERS, RHO) / 2 + 1 / MU
            #             print("SIM_TIME:",SIM_TIME)
            #             print("EXPECTED VALUES AND PROBABILITIES")

            #             print(f'Rho: {RHO}\nMu: {MU}\nLambda: {LAMBDA}\nExpected interarrival time: {1 / LAMBDA:.5f} time units')
            #             print(f'Expected processing time per server: {1 / MU:.5f} time units\n')
            #             print(f'Probability that a job has to wait: {pwait(SERVERS, RHO):.5f}')
            #             print(f'Expected queue length E(Lq): {expquel(SERVERS, RHO):.5f} customers\n')
            #             print(f'Expected waiting time E(W): {W:.5f} time units\n')
            #             E = expw(MU, SERVERS, RHO)

            time_idx += 1

    #             if W > TARGET_LATENCY / 1000:
    #                 LambdaUsed = True
    #                 CurrentLambdaJobs += int(TotalEventValues[0] / 2)
    #                 print("W:", W, "Violate Target Latency")

    if LambdaUsed:
        CurrentLambdaJobs = RemainEvents[0]

    CurrentInferentiaJobs = events[0] - CurrentLambdaJobs

    WorkedByLambda.append(CurrentLambdaJobs)
    WorkedByInferentia.append(CurrentInferentiaJobs)

    LambdaJobs += CurrentLambdaJobs
    InferentiaInstances += NeedInstances
    return LambdaJobs, InferentiaInstances


def Oracle(LambdaJobs, InferentiaInstances, events):
    CurrentLambdaJobs = 0
    event = events[0]
    NeedInstances = event // PerformInferentia[1]
    remainEvents = event % PerformInferentia[1]

    if PerformLambda[1] < event:
        NeedInstances += 1
    else:
        LambdaJobs += remainEvents
    InferentiaInstances += NeedInstances
    return LambdaJobs, InferentiaInstances


onlylambda_jobs = 0
max_instances = 0
scaling_usedinstances = 0
optimizesystem_jobs = [0, 0]
oracle_jobs = [0, 0]

start_time = 0

while (start_time <= end_time):
    remain_time = end_time - start_time
    last_time = T[-1]
    if last_time > remain_time:
        T = T[:-1]

    Events = RequestMonitor(start_time)

    TotalEventValues = np.array(list(Events.values()))

    onlylambda_jobs = OnlyLambda(onlylambda_jobs, TotalEventValues)
    max_instances = MaxInstance(max_instances, TotalEventValues)
    scaling_usedinstances = ScalingInstances(
        scaling_usedinstances, TotalEventValues)
    optimizesystem_jobs[0], optimizesystem_jobs[1] = OptimizeSystem(
        optimizesystem_jobs[0], optimizesystem_jobs[1], TotalEventValues)
    oracle_jobs[0], oracle_jobs[1] = Oracle(
        oracle_jobs[0], oracle_jobs[1], TotalEventValues)

    start_time += 1

print(onlylambda_jobs,
      max_instances,
      scaling_usedinstances,
      optimizesystem_jobs,
      oracle_jobs)

print('Only Lambda:', onlylambda_jobs * lambda_per_price)
print('Max Instances:', max_instances * (end_time + 1) * inferentia_price)
print('Scaling Instances:', scaling_usedinstances * inferentia_price)
print('Optimize System:', optimizesystem_jobs[0] * lambda_per_price +
      optimizesystem_jobs[1] * inferentia_price)
print('Oracle:', oracle_jobs[0] * lambda_per_second +
      oracle_jobs[1] * inferentia_price)
